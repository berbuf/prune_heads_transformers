{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1118 16:43:33.145685 140578894108480 file_utils.py:39] PyTorch version 1.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import modif_bert\n",
    "import transformers_modif\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1118 16:43:35.521104 140578894108480 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/bertrand/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1118 16:43:37.300052 140578894108480 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/bertrand/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1118 16:43:37.303895 140578894108480 configuration_utils.py:169] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1118 16:43:39.873214 140578894108480 modeling_utils.py:383] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/bertrand/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1118 16:43:44.397293 140578894108480 modeling_utils.py:456] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "weights = 'bert-base-uncased'\n",
    "tokenizer = transformers_modif.transformers.BertTokenizer.from_pretrained(weights)\n",
    "\n",
    "model = modif_bert.BertForMaskedLM.from_pretrained(weights)\n",
    "bert_model = model.bert\n",
    "language_model = model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'was', 'eating', 'a', 'sandwich', '.', '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"I was eating a sandwich.\", add_special_tokens=True)\n",
    "tok = [ tokenizer._convert_id_to_token(t) for t in ids ]\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sent(corpus):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for doc in corpus:\n",
    "        for sent in nlp(doc).sents:\n",
    "            yield list(zip(*[ [t.text, t.pos_, t.dep_] for t in sent ]))\n",
    "\n",
    "def mask_unk(text):\n",
    "    target_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    mask = [1] * len(target_ids[0])\n",
    "    for i in range(1, len(target_ids[0]) - 1):\n",
    "        if target_ids[i] == 100:   \n",
    "            mask[i] = 0\n",
    "    return mask\n",
    "\n",
    "def encode_ids(text):\n",
    "    target_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    tok = [ tokenizer._convert_id_to_token(t) for t in target_ids[0].numpy() ]\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    for index_target in range(1, len(input_ids[0]) - 1):\n",
    "        if target_ids[0][index_target] == 100:\n",
    "            continue\n",
    "        input_ids[0][index_target] = 103\n",
    "        yield input_ids, target_ids, index_target, tok\n",
    "        input_ids[0][index_target] = target_ids[0][index_target]\n",
    "    yield (), (), (), ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KD_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "def loss_distrib(prediction_scores, teacher):\n",
    "    input = prediction_scores[0][index_target].view(-1, prediction_scores.shape[2])\n",
    "    student = F.log_softmax(input, dim=-1)\n",
    "    return KD_loss(student, teacher)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "def loss_cross_entropy(prediction_scores, target_ids, index_target):\n",
    "    input = prediction_scores[0][index_target].view(-1, prediction_scores.shape[2])\n",
    "    target = target_ids[0][index_target].view(-1)\n",
    "    return loss(input, target)\n",
    "\n",
    "def ptokens(prediction_scores):\n",
    "    a = prediction_scores[0][index_target].view(-1, prediction_scores.shape[2])\n",
    "    d = np.argsort(-a[0].detach().numpy())\n",
    "    for i in d[:10]:\n",
    "        print (\"{:<20}{:<20}{}\".format(tokenizer._convert_id_to_token(i), i, a[0][i]))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm1(model):\n",
    "    norm_l1 = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            norm_l1 += param.norm(p=1)\n",
    "    return norm_l1\n",
    "\n",
    "def clip_value(model):\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data.clamp_(min=0., max=10.)\n",
    "\n",
    "def init_gates(model, lr):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.init_head_gates()\n",
    "    return optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "def get_target(target_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(target_ids)\n",
    "        prediction_scores = language_model(outputs[0])\n",
    "        #target_scores = prediction_scores[0][index_target].view(-1, prediction_scores.shape[2])\n",
    "        #teacher = F.softmax(target_scores, dim=-1)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gates_values(model):\n",
    "    gates = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    return np.array([ e.cpu().detach().numpy().flatten() for e in gates ])\n",
    "\n",
    "def round_gates(gates):\n",
    "    gates[gates < 0.5] = 0\n",
    "    gates[gates > 0.5] = 1\n",
    "    return gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"../corpus.txt\").readlines()\n",
    "gen_s = gen_sent(corpus)\n",
    "text, pos, dep = next(gen_s)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, target_ids, index_target, tokens = next(gen_ids)\n",
    "tokens[index_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 1045, 2572, 2005,  103, 1998, 1045, 2562, 2006, 3038, 2023,  102]]),\n",
       " tensor([[ 101, 1045, 2572, 2005, 7982, 1998, 1045, 2562, 2006, 3038, 2023,  102]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = [\"Here is the text to encode\", \"Next to me is the cat\", \"Generating sentences from a Continuous Space\",\n",
    "          \"Wikipedia is a free online website, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\",\n",
    "          \"She eats an apple\", \"An apple eats John\",\n",
    "          \"i am for dialogue and i keep on saying this\",\n",
    "          \"all four tasmanian boats have finished .\"]\n",
    "\n",
    "inp = [\"Here is [MASK] text to encode\",\n",
    "       \"Next to me is the [MASK]\", \n",
    "       \"Generating [MASK] from a continuous Space\",\n",
    "       \"Wikipedia is a free online [MASK], created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\",\n",
    "       \"She [MASK] an apple\",\n",
    "       \"An apple [MASK] John\",\n",
    "       \"i am for [MASK] and i keep on saying this\",\n",
    "       \"all four tasmanian [MASK] have finished .\"]\n",
    "\n",
    "sent = 6\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(inp[sent], add_special_tokens=True)])\n",
    "target_ids = torch.tensor([tokenizer.encode(target[sent], add_special_tokens=True)])\n",
    "\n",
    "index_target = 4\n",
    "\n",
    "# [MASK] = 103\n",
    "input_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer._convert_id_to_token(1012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 101,  101, 1045, 2572,  103, 1998, 1045, 2562, 2006, 3038, 2023,  102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 101, 1045, 2572, 2005, 7982, 1998, 1045, 2562, 2006, 3038, 2023,  102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue            7982                18.581022262573242\n",
      "dialogues           22580               12.793519973754883\n",
      "debate              5981                10.235848426818848\n",
      "reconciliation      16088               9.292250633239746\n",
      "reflection          9185                8.949795722961426\n",
      "unity               8499                8.89005184173584\n",
      "clarity             15563               8.853912353515625\n",
      "discussion          6594                8.655020713806152\n",
      "understanding       4824                8.214696884155273\n",
      "consensus           10465               7.789949417114258\n",
      "\n",
      "you                 2017                7.599301815032959\n",
      "real                2613                7.527132511138916\n",
      "nothing             2498                6.66353702545166\n",
      "sale                5096                6.581860065460205\n",
      "him                 2032                6.0994367599487305\n",
      "her                 2014                5.7508039474487305\n",
      "sure                2469                5.581497669219971\n",
      "them                2068                5.4741082191467285\n",
      "it                  2009                5.393154144287109\n",
      "good                2204                5.228878498077393\n",
      "\n",
      "10.994763374328613                      144.0\n",
      "9.947410583496094                       135.00009155273438\n",
      "8.852987289428711                       126.15587615966797\n",
      "7.7896904945373535                      117.2755355834961\n",
      "6.798891067504883                       108.52137756347656\n",
      "5.946288108825684                       99.92491912841797\n",
      "5.337142467498779                       91.40198516845703\n",
      "4.834772109985352                       82.9751968383789\n",
      "4.284390449523926                       74.70730590820312\n",
      "3.644460678100586                       66.74143981933594\n",
      "2.938288688659668                       59.34667205810547\n",
      "2.6314334869384766                      57.2945671081543\n",
      "2.3036561012268066                      56.97257995605469\n",
      "1.9410619735717773                      57.0212287902832\n",
      "1.6054792404174805                      57.21746063232422\n",
      "1.3353691101074219                      57.287540435791016\n",
      "1.1087646484375                         57.17948532104492\n",
      "0.9139032363891602                      56.84859085083008\n",
      "0.7672538757324219                      56.39414978027344\n",
      "0.6634330749511719                      55.80119323730469\n",
      "0.6011695861816406                      55.16371154785156\n",
      "0.5625696182250977                      54.53457260131836\n",
      "0.5344448089599609                      53.80581283569336\n",
      "0.5020713806152344                      53.09587097167969\n",
      "0.5025920867919922                      52.2499885559082\n",
      "0.5076045989990234                      51.405677795410156\n",
      "0.5153121948242188                      50.57484817504883\n",
      "0.5406036376953125                      49.71306228637695\n",
      "0.5727558135986328                      48.796627044677734\n",
      "0.6010046005249023                      47.86992645263672\n",
      "0.6197042465209961                      46.93413543701172\n",
      "0.6337251663208008                      46.02154541015625\n",
      "0.635930061340332                       45.117366790771484\n",
      "0.6264171600341797                      44.14959716796875\n",
      "0.6672449111938477                      43.392555236816406\n",
      "0.6450986862182617                      42.728214263916016\n",
      "0.5835866928100586                      42.099578857421875\n",
      "0.5805149078369141                      41.605045318603516\n",
      "0.5747270584106445                      41.21300506591797\n",
      "0.5658693313598633                      40.81737518310547\n",
      "0.5553760528564453                      40.352996826171875\n",
      "0.5379972457885742                      39.90668869018555\n",
      "0.5129289627075195                      39.511924743652344\n",
      "0.4957304000854492                      39.01673126220703\n",
      "0.4874448776245117                      38.44528579711914\n",
      "0.5085592269897461                      37.885135650634766\n",
      "0.5441827774047852                      37.11360168457031\n",
      "0.5590505599975586                      36.4080924987793\n",
      "0.5340700149536133                      35.88593292236328\n",
      "0.5141410827636719                      35.46799087524414\n",
      "\n",
      "dialogue            7982                10.736623764038086\n",
      "speech              4613                7.367282867431641\n",
      "criticism           6256                7.132911205291748\n",
      "discussion          6594                6.767677307128906\n",
      "speeches            13867               6.758157253265381\n",
      "debate              5981                6.505148887634277\n",
      "conversation        4512                6.304672718048096\n",
      "prayer              7083                6.26190185546875\n",
      "words               2616                6.170753002166748\n",
      "peace               3521                6.10114049911499\n",
      "\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "sigma = 0.1\n",
    "epochs = 50\n",
    "\n",
    "optimizer = init_gates(bert_model, lr)\n",
    "target_scores = get_target(target_ids)\n",
    "\n",
    "ptokens(target_scores)\n",
    "ptokens(language_model(bert_model(input_ids)[0]))\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    prediction_scores = language_model(bert_model(input_ids)[0])\n",
    "    l = loss_cross_entropy(prediction_scores, target_ids, index_target)\n",
    "    n = norm1(bert_model)\n",
    "\n",
    "    print (\"{:<40}{}\".format(l.item(), n.item()))\n",
    "\n",
    "    l += (sigma * n)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    clip_value(bert_model)\n",
    "\n",
    "print ()\n",
    "ptokens(prediction_scores)\n",
    "gates = gates_values(bert_model)\n",
    "print (sum(round_gates(gates.copy().flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(gates)\n",
    "plt.show()\n",
    "plt.matshow(r_gates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 2005, 7982, 1998, 1045, 2562, 2006, 3038, 2023,  102]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
